{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "401347e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\nouam\\anaconda3\\lib\\site-packages (0.26.5)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from openai) (4.59.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2020.12.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.10)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from aiohttp->openai) (20.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c9a7d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\nouam\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: blobfile>=2 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from tiktoken) (2.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from tiktoken) (2022.10.31)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from tiktoken) (2.28.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from blobfile>=2->tiktoken) (1.26.4)\n",
      "Requirement already satisfied: filelock~=3.0 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from blobfile>=2->tiktoken) (3.0.12)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from blobfile>=2->tiktoken) (3.17)\n",
      "Requirement already satisfied: lxml~=4.9 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from blobfile>=2->tiktoken) (4.9.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2020.12.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nouam\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9cb2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "SECRET_KEY=\"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\"\n",
    "#ORG_ID= \"org-Nb4XRWRLbNL4xvPpV7Cv9lSD\"\n",
    "\n",
    "\n",
    "openai.api_key = SECRET_KEY\n",
    "prt= \"Classify the sentiment in these tweets:\\n I cannot stand homework \\n This sucks.I am bored üò† \\n I cannot wait for Halloween!!! \\n My cat is adorable ‚ù§Ô∏è‚ù§Ô∏è\\n I hate chocolate \\n Tweet sentiment ratings:\"\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93985f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'p50k_base'>\n"
     ]
    }
   ],
   "source": [
    "print(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1f7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef73f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## send Request to api\n",
    "response = openai.Completion.create(\n",
    "model=\"text-davinci-003\",\n",
    "prompt=prt,\n",
    "temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6d6508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n1. Negative \\n2. Negative \\n3. Positive \\n\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1678208389,\n",
      "  \"id\": \"cmpl-6rUxBSZ7Xi714IachjP6VK185w688\",\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 16,\n",
      "    \"prompt_tokens\": 53,\n",
      "    \"total_tokens\": 69\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Response \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29077a",
   "metadata": {},
   "source": [
    "### SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15870905",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"code-davinci-002\",\n",
    "  prompt=\"### Postgres SQL tables, with their properties:\\n#\\n# Employee(id, name, department_id)\\n# Department(id, name, address)\\n# Salary_Payments(id, employee_id, amount, date)\\n#\\n### A query to list the names of the departments which employed more than 10 employees in the last 3 months\\nSELECT\",\n",
    "  temperature=0,\n",
    "  max_tokens=150,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"#\", \";\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4c3c7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" DISTINCT department.name\\nFROM department\\nINNER JOIN employee ON department.id = employee.department_id\\nINNER JOIN salary_payments ON employee.id = salary_payments.employee_id\\nWHERE salary_payments.date >= (CURRENT_DATE - INTERVAL '3 months')\\nGROUP BY department.name\\nHAVING COUNT(employee.id) > 10\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1677502837,\n",
      "  \"id\": \"cmpl-6oXPJx32lng3uXURq1tUYRcQtp0Z9\",\n",
      "  \"model\": \"code-davinci-002\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 93,\n",
      "    \"prompt_tokens\": 77,\n",
      "    \"total_tokens\": 170\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Response \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6f9ce",
   "metadata": {},
   "source": [
    "### Factual answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f280b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"Q: Who is Batman?\\nA: Batman is a fictional comic book character.\\n\\nQ: What is torsalplexity?\\nA: ?\\n\\nQ: What is Devz9?\\nA: ?\\n\\nQ: Who is George Lucas?\\nA: George Lucas is American film director and producer famous for creating Star Wars.\\n\\nQ: What is the capital of California?\\nA: Sacramento.\\n\\nQ: What orbits the Earth?\\nA: The Moon.\\n\\nQ: Who is Fred Rickerson?\\nA: ?\\n\\nQ: What is an atom?\\nA: An atom is a tiny particle that makes up everything.\\n\\nQ: Who is Alvan Muntz?\\nA: ?\\n\\nQ: What is Kozar-09?\\nA: ?\\n\\nQ: How many moons does Mars have?\\nA: Two, Phobos and Deimos.\\n\\nQ: What's a language model?\\nA:\",\n",
    "  temperature=0,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0.\n",
    "  stop=[\"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b37cdbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" A language model is a type of artificial intelligence that uses statistical techniques to predict the probability of a sequence of words.\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1677504382,\n",
      "  \"id\": \"cmpl-6oXoEMZapqwMipSM9o6brYzIMaj8U\",\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 23,\n",
      "    \"prompt_tokens\": 208,\n",
      "    \"total_tokens\": 231\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Response \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1ba25",
   "metadata": {},
   "source": [
    "## Experiment -- text-davinci--002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b3649f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "SECRET_KEY=\"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\"\n",
    "openai.api_key = SECRET_KEY\n",
    "\n",
    "prompts = [\n",
    "   \"What is the meaning of life?\",\n",
    "    \"How do I make a chocolate cake?\",\n",
    "    \"What is the capital of France?\", \n",
    "]\n",
    "\n",
    "model_engine = \"text-davinci-002\" # or your preferred model engine\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompts,\n",
    "    max_tokens=50,\n",
    "    n=1,\n",
    "    #stop=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed4c8d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\nThere is no one meaning of life.\"\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\nThere are multiple ways to make a chocolate cake, as there are many chocolate cake recipes. A few popular recipes include the German chocolate cake, molten lava cake, and chocolate Bundt cake. To make a chocolate cake from scratch, mix together\"\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 2,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\nThe capital of France is Paris.\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1678264210,\n",
      "  \"id\": \"cmpl-6rjTWyKLxlZI0h3KpPim5eOPmqhQ3\",\n",
      "  \"model\": \"text-davinci-002\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 69,\n",
      "    \"prompt_tokens\": 22,\n",
      "    \"total_tokens\": 91\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12d9c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There is no one meaning of life.\n",
      "\n",
      "\n",
      "There are multiple ways to make a chocolate cake, as there are many chocolate cake recipes. A few popular recipes include the German chocolate cake, molten lava cake, and chocolate Bundt cake. To make a chocolate cake from scratch, mix together\n",
      "\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "for choice in response.choices:\n",
    "    print(choice.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3c6f1",
   "metadata": {},
   "source": [
    "## Experiment -- text-davinci--003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "637abbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_KEY=\"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\"\n",
    "openai.api_key = SECRET_KEY\n",
    "\n",
    "prompts = [\n",
    "   \"What is the meaning of life?\",\n",
    "    \"How do I make a chocolate cake?\",\n",
    "    \"What is the capital of France?\", \n",
    "]\n",
    "\n",
    "model_engine = \"text-davinci-003\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompts,\n",
    "    max_tokens=50,\n",
    "    n=1,\n",
    "    #stop=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4ce69da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The meaning of life is subjective and can depend on many different factors. Ultimately, it is up to each individual to decide what the true meaning of life is for them.\n",
      "\n",
      "\n",
      "Ingredients\n",
      "\n",
      "- 2 cups all-purpose flour\n",
      "- 2 cups sugar\n",
      "- 3/4 cup cocoa powder\n",
      "- 2 teaspoons baking soda\n",
      "- 1 teaspoon salt\n",
      "- 2 eggs\n",
      "- 1 cup whole milk\n",
      "- 1/\n",
      "\n",
      "\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "for choice in response.choices:\n",
    "    print(choice.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a2725",
   "metadata": {},
   "source": [
    "### Experiment Chatgpt -- text-davinci-- 003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a412adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_KEY = \"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\"\n",
    "openai.api_key = SECRET_KEY \n",
    "\n",
    "prompts = [\n",
    "    \"15+15 = \",\n",
    "    \"16+18 = \",\n",
    "    \"2000+20000 = \",\n",
    "    \"1/100000 = \"\n",
    "]\n",
    "\n",
    "model_engine = \"text-davinci-003\"\n",
    "response = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompts,\n",
    "    max_tokens=50,\n",
    "    n=1,\n",
    "    #stop=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dad7365f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\n30\"\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\n34\"\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 2,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\n22000\"\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 3,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" 0.00001\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1678264732,\n",
      "  \"id\": \"cmpl-6rjbwYMNbuh1whmBEI2Pis76Ww94Y\",\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 14,\n",
      "    \"prompt_tokens\": 22,\n",
      "    \"total_tokens\": 36\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1b616d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "30\n",
      "\n",
      "\n",
      "34\n",
      "\n",
      "\n",
      "22000\n",
      " 0.00001\n"
     ]
    }
   ],
   "source": [
    "for choice in response.choices:\n",
    "    print(choice.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c961ee2",
   "metadata": {},
   "source": [
    "### Experiment Chatgpt -- text-davinci -- 002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "345519f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_KEY = \"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\"\n",
    "openai.api_key = SECRET_KEY \n",
    "\n",
    "prompts = [\n",
    "    \"15+15 = \",\n",
    "    \"16+18 = \",\n",
    "    \"2000+20000 = \",\n",
    "    \"1/100000 = \"\n",
    "]\n",
    "\n",
    "model_engine = \"text-davinci-002\"\n",
    "response = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompts,\n",
    "    max_tokens=50,\n",
    "    n=1,\n",
    "    #stop=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cc9f6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"ertu\\n\\t\\t\\t}\\n\\t\\t\\techo \\\"<script>swal('Sukses', 'Data : \\\".$totalDetail[0]['nama'].\\\" Saved!', 'success');\\n\\t\\t\\t\\t\"\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" 34\\n */\\n#endif\\n\\n/**\\n * quantization parameters\\n */\\n\\n/* quantization weights access */\\n#define FF_WFT_ antivq_tables[ff_wft_index]\\nextern const short\"\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 2,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\n30000\"\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 3,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" 0.00001\\\\n']\\n\\n \\n\\nclass Response(object):\\n\\n    def __init__(self, site, status, body=None, headers=None, request=None,\\n                 error=None, **kwargs\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1678266090,\n",
      "  \"id\": \"cmpl-6rjxqXUzuFgUNirzWUIKfsHx6UJY3\",\n",
      "  \"model\": \"text-davinci-002\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 154,\n",
      "    \"prompt_tokens\": 22,\n",
      "    \"total_tokens\": 176\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63e6f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ertu\n",
      "\t\t\t}\n",
      "\t\t\techo \"<script>swal('Sukses', 'Data : \".$totalDetail[0]['nama'].\" Saved!', 'success');\n",
      "\t\t\t\t\n",
      " 34\n",
      " */\n",
      "#endif\n",
      "\n",
      "/**\n",
      " * quantization parameters\n",
      " */\n",
      "\n",
      "/* quantization weights access */\n",
      "#define FF_WFT_ antivq_tables[ff_wft_index]\n",
      "extern const short\n",
      "\n",
      "\n",
      "30000\n",
      " 0.00001\\n']\n",
      "\n",
      " \n",
      "\n",
      "class Response(object):\n",
      "\n",
      "    def __init__(self, site, status, body=None, headers=None, request=None,\n",
      "                 error=None, **kwargs\n"
     ]
    }
   ],
   "source": [
    "for choice in response.choices:\n",
    "    print(choice.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30380d48",
   "metadata": {},
   "source": [
    "#### Zahlen Generierung  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "f1c2a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1692 1302 8245 1556 1952 6629 1454 7727 3884 1453]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_generator (size,base):\n",
    "    random_numbers = np.random.randint(1, base, size=size,dtype=np.int64)\n",
    "    return random_numbers\n",
    "random_numbers = random_generator(10,10**4)\n",
    "print(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "9f8f7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_next_elements(lst):\n",
    "    return [(lst[i] + lst[(i+1) % len(lst)]) for i in range(len(lst))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980dcf0",
   "metadata": {},
   "source": [
    "#### Prompts generierung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "333ce34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex:\n",
    "# prompts = [\n",
    "#     \"15+15 = \",\n",
    "#     \"16+18 = \",\n",
    "#     \"2000+20000 = \",\n",
    "#     \"1/100000 = \"\n",
    "# ]\n",
    "#signs = {\"/\",\"+\",\"-\"}\n",
    "def prompts_builder(random_numbers):\n",
    "    prompts = [f\"{a} + {b} =\\n\" for a, b in zip(random_numbers, random_numbers[1:])]\n",
    "    ground_truth =[(random_numbers[i] + random_numbers[(i+1) % len(random_numbers)]) for i in range(len(random_numbers))]\n",
    "    return prompts, ground_truth[:9]\n",
    "\n",
    "prompts, ground_truth = prompts_builder(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "4266d43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1692, 1302), (1302, 8245), (8245, 1556), (1556, 1952), (1952, 6629), (6629, 1454), (1454, 7727), (7727, 3884), (3884, 1453)]\n"
     ]
    }
   ],
   "source": [
    "my_tuples = [(random_numbers[i],random_numbers[i+1]) for i in range(len(random_numbers)-1)]\n",
    "print(my_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "3d08b196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: ['1692 + 1302 =\\n', '1302 + 8245 =\\n', '8245 + 1556 =\\n', '1556 + 1952 =\\n', '1952 + 6629 =\\n', '6629 + 1454 =\\n', '1454 + 7727 =\\n', '7727 + 3884 =\\n', '3884 + 1453 =\\n'] ground_truth: [2994, 9547, 9801, 3508, 8581, 8083, 9181, 11611, 5337]\n"
     ]
    }
   ],
   "source": [
    "print(f\"prompts: {prompts}\", f\"ground_truth: {ground_truth[:9]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "6845331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: 9 ground_truth: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"prompts: {len(prompts)}\", f\"ground_truth: {len(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "a8cf6953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2994, 9547, 9801, 3508, 8581, 8083, 9181, 11611, 5337]\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ceb63a",
   "metadata": {},
   "source": [
    "#### API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "ef80f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_KEY = \"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\"\n",
    "openai.api_key = SECRET_KEY "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04742469",
   "metadata": {},
   "source": [
    "#### Response -- text-davinci--002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "aef461b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_engine = \"text-davinci-002\"\n",
    "response = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompts,\n",
    "    max_tokens=50,\n",
    "    n=1,\n",
    "    #stop=None,\n",
    "   # temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "c4bf83b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "680\n",
      "\n",
      "746\n",
      "            # 990.\n",
      "            #\n",
      "            # Find the sum of all products whose multiplicand/multiplier/product\n",
      "            # identity can be written as a 1 through 9 pandigital.\n",
      "#===============================================================================\n",
      "\n",
      "from\n",
      "\n",
      "1444\n",
      "\n",
      "1526\n",
      "\n",
      "1333\n",
      "\n",
      "In this example, we see our facts (862 and 481), we've set up our operation (addition) between two of them, and we've produced our answer (1333).\n",
      "python2 1473 + 1800 =\n",
      "python2 1693 + 1877 =\n",
      "python2 1949 + 2307 =\n",
      "python2 1573 + 2122 =\n",
      "python2 1807 + 2076 =\n",
      "python2 1351 + 1607\n",
      "1832 + 9682 =\n",
      "115340\n",
      "</code>\n",
      "The program will receive a number on the console:\n",
      "<code>C:\\&gt; reverse 1\n",
      "Input: 123\n",
      "Input: 456\n",
      "Input: 789\n",
      "\n",
      "1474\n"
     ]
    }
   ],
   "source": [
    "for choice in response.choices:\n",
    "    print(choice.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e0ce12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "15+15 = 30\n",
      "\n",
      "\n",
      "16 + 18 = 34\n",
      "\n",
      "\n",
      "22000\n",
      "‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏(0,1 %) ‡∏ô\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list(map(lambda choice: print(chdoice.text), response.choices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8c237",
   "metadata": {},
   "source": [
    "#### Response --text-davinci--003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "3164e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_engine = \"text-davinci-003\"\n",
    "response = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompts,\n",
    "    #max_tokens=0,\n",
    "    #n=1,\n",
    "    #stop=None,\n",
    "    temperature = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "3a4812ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "922eb806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2994, 9090, 9701, 35, 8581, 7783, 9481, 11761, 5337]\n"
     ]
    }
   ],
   "source": [
    "choices = [int(choice.text) for choice in response.choices]\n",
    "print(choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453dc65",
   "metadata": {},
   "source": [
    "### Abweichung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "d440b274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, -457, -100, -3473, 0, -300, 300, 150, 0]"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(response.choices))\n",
    "abweichungen =  [int(response.choices[i].text) - ground_truth[i] for i in range(len(response.choices))]\n",
    "abweichungen\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd12cc2",
   "metadata": {},
   "source": [
    "### summarization in DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "cce326bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-08 13:59:55.704864\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "# dates = [now for _ in range(len(prompts))]\n",
    "# print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "69c20e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "for el in range(len(prompts)):\n",
    "    dates.append(now.strftime('%Y-%m-%d'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "89320d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-03-08',\n",
       " '2023-03-08',\n",
       " '2023-03-08',\n",
       " '2023-03-08',\n",
       " '2023-03-08',\n",
       " '2023-03-08',\n",
       " '2023-03-08',\n",
       " '2023-03-08',\n",
       " '2023-03-08']"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "27ffae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=[\"s1\",\"s2\",\"AiErg\",\"Erg\",\"Abweichung\",\"Datum\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "a81ce97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"s1\"] = [t[0] for t in my_tuples]\n",
    "df[\"s2\"] = [t[1] for t in my_tuples]\n",
    "df[\"Datum\"]  = dates\n",
    "df[\"Erg\"] = ground_truth\n",
    "df[\"Abweichung\"] = abweichungen\n",
    "df[\"AiErg\"] = choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "451c018f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>AiErg</th>\n",
       "      <th>Erg</th>\n",
       "      <th>Abweichung</th>\n",
       "      <th>Datum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1692</td>\n",
       "      <td>1302</td>\n",
       "      <td>2994</td>\n",
       "      <td>2994</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1302</td>\n",
       "      <td>8245</td>\n",
       "      <td>9090</td>\n",
       "      <td>9547</td>\n",
       "      <td>-457</td>\n",
       "      <td>2023-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8245</td>\n",
       "      <td>1556</td>\n",
       "      <td>9701</td>\n",
       "      <td>9801</td>\n",
       "      <td>-100</td>\n",
       "      <td>2023-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1556</td>\n",
       "      <td>1952</td>\n",
       "      <td>35</td>\n",
       "      <td>3508</td>\n",
       "      <td>-3473</td>\n",
       "      <td>2023-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1952</td>\n",
       "      <td>6629</td>\n",
       "      <td>8581</td>\n",
       "      <td>8581</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6629</td>\n",
       "      <td>1454</td>\n",
       "      <td>7783</td>\n",
       "      <td>8083</td>\n",
       "      <td>-300</td>\n",
       "      <td>2023-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1454</td>\n",
       "      <td>7727</td>\n",
       "      <td>9481</td>\n",
       "      <td>9181</td>\n",
       "      <td>300</td>\n",
       "      <td>2023-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7727</td>\n",
       "      <td>3884</td>\n",
       "      <td>11761</td>\n",
       "      <td>11611</td>\n",
       "      <td>150</td>\n",
       "      <td>2023-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3884</td>\n",
       "      <td>1453</td>\n",
       "      <td>5337</td>\n",
       "      <td>5337</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     s1    s2  AiErg    Erg  Abweichung       Datum\n",
       "0  1692  1302   2994   2994           0  2023-03-08\n",
       "1  1302  8245   9090   9547        -457  2023-03-08\n",
       "2  8245  1556   9701   9801        -100  2023-03-08\n",
       "3  1556  1952     35   3508       -3473  2023-03-08\n",
       "4  1952  6629   8581   8581           0  2023-03-08\n",
       "5  6629  1454   7783   8083        -300  2023-03-08\n",
       "6  1454  7727   9481   9181         300  2023-03-08\n",
       "7  7727  3884  11761  11611         150  2023-03-08\n",
       "8  3884  1453   5337   5337           0  2023-03-08"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "bd9e3e30",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-471-b9a49e82cccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m response = openai.Completion.create(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_engine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprompts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    151\u001b[0m         )\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         )\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m             return (\n\u001b[1;32m--> 619\u001b[1;33m                 self._interpret_response_line(\n\u001b[0m\u001b[0;32m    620\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m             raise self.handle_error_response(\n\u001b[0m\u001b[0;32m    680\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m             )\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?"
     ]
    }
   ],
   "source": [
    "model_engine = \"gpt-3.5-turbo\"\n",
    "response = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompts,\n",
    "    max_tokens=50,\n",
    "    n=1,\n",
    "    #stop=None,\n",
    "   # temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f6c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "for choice in response.choices:\n",
    "    print(choice.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
